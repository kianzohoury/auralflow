# Model design.
model:
  # name:
  #   Model name or tag.
  # base-model:
  #   Base model architecture to inherit from.
  # depth:
  #   Depth or height of the model (only affects Unet).
  # bottleneck:
  #   type:
  #     Type of bottleneck layers. Options: [conv|lstm|fc]
  #   layers:
  #     Depth of the stack of layers that make up the bottleneck portion.
  # encoder:
  #   block-layers:
  #     Number of conv layers that make up a single encoder block.
  #   kernel-size:
  #     Size of the kernels.
  #   down:
  #     Downsampling method. Options: [conv|maxpool|avgpool|downsample|decimate]
  #   leak:
  #     Leakiness factor of ReLU activations between encoder conv layers.
  # decoder:
  #   block-layers:
  #     Number of conv layers that make up a single decoder block.
  #   kernel-size:
  #     Size of the kernels.
  #   up:
  #     Upsampling method. Options: [transposed|upsample|unmaxpool]
  #   dropout:
  #     Dropout probability.
  #   num-dropouts:
  #     Number of dropout layers beginning from the first decoder block.
  #   skip:
  #     Whether to include skip connections or not (recommended).
  # mask-activation:
  #   Final activation for producing the mask.
  # input-normalization:
  #   Learnable normalization parameters to apply before forward pass.
  # output-normalization:
  #   Learnable normalization parameters to apply before final mask activation.
  name: my_model
  base-model: demucs
  max-layers: 6
  init-features: 16
  bottleneck:
    type: conv
    layers: 1
  encoder:
    block-layers: 1
    kernel-size: 5
    down: conv
    leak: 0.2
  decoder:
    block-layers: 1
    kernel-size: 5
    up: transposed
    dropout: 0.5
  num-dropouts: 3
  skip-connections: True
  mask-activation: sigmoid
  input-norm: False
  output-norm: False

  layers: [conv_8, relu, conv_1, glu]